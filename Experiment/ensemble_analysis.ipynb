{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "048305c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "340cb1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds = pd.read_excel('final_predictions/deepseek14_all_pred.xlsx', index_col=0)\n",
    "df_llama = pd.read_excel('final_predictions/llama8_all_pred.xlsx', index_col=0)\n",
    "df_gemma = pd.read_excel('final_predictions/gemma12_all_pred.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40a2a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_labels = [col for col in df_ds.columns if col.startswith('label')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76c9c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_preds(df_ds, df_llama, df_gemma, task, column_label, column_real):\n",
    "    \n",
    "    df = pd.merge(pd.merge(df_ds[df_ds['Task'] == task][['RequirementText',column_real, column_label]], df_llama[df_llama['Task'] == task][['RequirementText',column_real,column_label]], \n",
    "            how = 'inner', on = ['RequirementText',  column_real], suffixes=('_DS', '_LM'))\n",
    "            , df_gemma[df_gemma['Task'] == task][['RequirementText',column_real, column_label]], how = 'inner', on = ['RequirementText', column_real])\n",
    "\n",
    "    df.rename(columns={column_label: column_label+ '_GM'}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bdb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_votes ={}\n",
    "all_scores = {'f1': [], 'precision': [], 'recall': [], 'specificity':[]}\n",
    "\n",
    "for dataset in ['l', 'p', 'rds', 'o']:\n",
    "    for task in ['f','q']:\n",
    "        \n",
    "        \n",
    "        for i in [1,2,4,8,16,32,64]: \n",
    "            column_label = 'label_' + str(i)\n",
    "\n",
    "            if task == 'f': column_real = 'IsFunctional'\n",
    "            else: column_real = 'IsQuality'\n",
    "            df1 = merge_preds(df_ds[df_ds['Dataset'] == dataset], df_llama[df_llama['Dataset'] == dataset], df_gemma[df_gemma['Dataset'] == dataset], task, column_label, column_real)\n",
    "            pred = df1[[column_label+m_name for m_name in ['_DS', '_LM', '_GM']]].mode(axis=1)[0].to_list()\n",
    "            majority_votes[column_label] = pred\n",
    "\n",
    "            tn, fp, fn, tp = confusion_matrix(df1[column_real], pred).ravel()\n",
    "\n",
    "            precision = precision_score(df1[column_real], pred)\n",
    "            recall = recall_score(df1[column_real], pred)\n",
    "            f1 = f1_score(df1[column_real], pred)\n",
    "            all_scores['specificity'].append({'task': task, 'size': i, 'dataset': dataset,'score': tn / (tn+fp)})\n",
    "            all_scores['f1'].append({'size': i, 'task': task, 'score': f1, 'dataset': dataset})\n",
    "            all_scores['precision'].append({'size': i, 'task': task, 'score': precision, 'dataset': dataset})\n",
    "            all_scores['recall'].append({'size': i, 'task': task, 'score': recall, 'dataset': dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cbcd79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_f1 = pd.DataFrame(all_scores['f1'])\n",
    "maj_precision = pd.DataFrame(all_scores['precision'])\n",
    "maj_recall = pd.DataFrame(all_scores['recall'])\n",
    "maj_spec = pd.DataFrame(all_scores['specificity'])\n",
    "maj_f1['model'] = ['ensemble'] *len(maj_f1)\n",
    "maj_precision['model'] = ['ensemble'] *len(maj_precision)\n",
    "maj_recall['model'] = ['ensemble'] *len(maj_recall)\n",
    "maj_spec['model'] = ['ensemble'] *len(maj_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8dda7c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precision = pd.read_excel('final_predictions/all_precision_score.xlsx', index_col=[0])\n",
    "df_recall = pd.read_excel('final_predictions/all_recall_score.xlsx', index_col=[0])\n",
    "df_f1 = pd.read_excel('final_predictions/all_f1_score.xlsx', index_col=[0])\n",
    "df_spec = pd.read_excel('final_predictions/all_specificity_score.xlsx', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73869b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_table(df, df_maj, task = 'q'):\n",
    "    df = df[df_maj.columns]\n",
    "    df = pd.concat([df, df_maj])\n",
    "    grouped_metrics =df[df['task'] == task][['size', 'model', 'score', 'task', 'dataset']].groupby(['size', 'model'])['score'].agg({'mean', 'std'})\n",
    "    grouped_metrics.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in grouped_metrics.columns]\n",
    "\n",
    "    grouped = round(grouped_metrics,3).reset_index()\n",
    "    pivot_df = grouped.pivot(index='size', columns='model', values=['mean', 'std'])\n",
    "    pivot_df.columns = pivot_df.columns.swaplevel(0, 1)\n",
    "    pivot_df = pivot_df.sort_index(axis=1, level=0)\n",
    "    return pivot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc062732",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = [('deepseek14','mean'),\n",
    "            ('deepseek14', 'std'),\n",
    "            ('gemma12','mean'),\n",
    "            ('gemma12', 'std'),\n",
    "            ('llama8','mean'),\n",
    "            ('llama8', 'std'), \n",
    "            ('ensemble','mean'),\n",
    "            ('ensemble','std')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b5645a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_specificity = get_metrics_table(df_spec, maj_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9bc00bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_precision = get_metrics_table(df_precision, maj_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f588c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_recall = get_metrics_table(df_recall, maj_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3a9d68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('tables_compare_ensemble.xlsx', engine='xlsxwriter') as writer:\n",
    "    table_specificity[column_order].to_excel(writer, sheet_name='Specificity')\n",
    "    table_precision[column_order].to_excel(writer, sheet_name='Precision')\n",
    "    table_recall[column_order].to_excel(writer, sheet_name='Recall')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_usst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
