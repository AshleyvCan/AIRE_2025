{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "048305c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "340cb1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds = pd.read_excel('final_predictions/deepseek14_all_pred.xlsx', index_col=0)\n",
    "df_llama = pd.read_excel('final_predictions/llama8_all_pred.xlsx', index_col=0)\n",
    "df_gemma = pd.read_excel('final_predictions/gemma12_all_pred.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "40a2a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_labels = [col for col in df_ds.columns if col.startswith('label')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5656414",
   "metadata": {},
   "source": [
    "## Determine the number of correct and incorrect predictions given the batch size, task, model, and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stats_pred(df_ds, df_llama, df_gemma, task):\n",
    "    df_all = {}\n",
    "    df_3 = []\n",
    "    selected_data = ['l', 'p', 'o', 'rds']\n",
    "    model_names = []\n",
    "    for i, df in enumerate([df_ds.reset_index(), df_llama.reset_index(), df_gemma.reset_index()]):\n",
    "        model_names.append(df.iloc[0]['model'][:4])\n",
    "            \n",
    "        if task == 'f': real_col = 'IsFunctional'\n",
    "        else: real_col = 'IsQuality'\n",
    "        df_task = df[(df['Task'] == task) & (df['Dataset'].isin(selected_data))].copy()\n",
    "\n",
    "        df_task.loc[:,'both'] = [1 if row['IsFunctional'] + row['IsQuality'] == 2 else 0 for i, row in df_task.iterrows()]\n",
    "        \n",
    "\n",
    "        df_task['correct_1'] = [1 if row['label_1'] == row[real_col] else 0 for i, row in df_task.iterrows()]\n",
    "        df_task['correct_2'] = [1 if row['label_2'] == row[real_col] else 0 for i, row in df_task.iterrows()]\n",
    "        df_task['correct_4'] = [1 if row['label_4'] == row[real_col] else 0 for i, row in df_task.iterrows()]\n",
    "        df_task['correct_8'] = [1 if row['label_8'] == row[real_col] else 0 for i, row in df_task.iterrows()]\n",
    "        df_task['correct_16'] = [1 if row['label_16'] == row[real_col] else 0 for i, row in df_task.iterrows()]\n",
    "        df_task['correct_32'] = [1 if row['label_32'] == row[real_col] else 0 for i, row in df_task.iterrows()]\n",
    "        df_task['correct_64'] = [1 if row['label_64'] == row[real_col] else 0 for i, row in df_task.iterrows()]\n",
    "        if len(df_3) == 0:\n",
    "            df_3 = df_task[['index',real_col,'RequirementText',  'both',\n",
    "                            'Dataset', 'correct_1', 'correct_2','correct_4','correct_8','correct_16','correct_32','correct_64']]\n",
    "                \n",
    "        else:\n",
    "            df_3 = pd.merge(df_3, df_task[['index','RequirementText', 'correct_1', 'correct_2','correct_4','correct_8','correct_16','correct_32','correct_64']], on = ['index', 'RequirementText'], suffixes=('_'+ model_names[i-1], '_'+ model_names[i]))\n",
    "    return df_3\n",
    "\n",
    "def agg_stats(df):\n",
    "    batch_sizes = [1,2,4,8,16,32,64]\n",
    "    all_stats = []\n",
    "    for m in ['_deep', '_meta', '']:\n",
    "        all_stats_per_model = []\n",
    "        for bs in batch_sizes:\n",
    "            df_i = df.groupby(['correct_' + str(bs)+ m, 'Dataset']).agg({'both': 'sum', 'RequirementText':'count'}).reset_index()\n",
    "            df_i['type'] = bs\n",
    "\n",
    "            if m == '':\n",
    "                df_i['model'] = 'gemma'\n",
    "            else:\n",
    "                df_i['model'] = m[1:]\n",
    "            \n",
    "            df_i = df_i.rename(columns={'correct_' + str(bs)+ m: 'correct'})\n",
    "            all_stats_per_model.append(df_i)\n",
    "           \n",
    "        df_stats_per_model = pd.concat(all_stats_per_model) \n",
    "       \n",
    "        df_stats_per_model['ratio'] = df_stats_per_model['both']/df_stats_per_model['RequirementText']\n",
    "\n",
    "        all_stats.append(df_stats_per_model)\n",
    "\n",
    "    return pd.concat(all_stats)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "724e3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q = get_stats_pred(df_ds, df_llama, df_gemma, 'q')\n",
    "df_stats_quality = agg_stats(df_q)\n",
    "df_f = get_stats_pred(df_ds, df_llama, df_gemma, 'f')\n",
    "df_stats_functional = agg_stats(df_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa48d6d",
   "metadata": {},
   "source": [
    "## Calculate the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60301ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_per_dataset = df_f.groupby('Dataset')['both'].aggregate(['sum', 'count']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_per_dataset['ratio_total'] = ratio_per_dataset['sum']/ratio_per_dataset['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8c2539fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_functional = df_stats_functional.merge(ratio_per_dataset[['Dataset', 'ratio_total']], on = 'Dataset')\n",
    "df_stats_quality = df_stats_quality.merge(ratio_per_dataset[['Dataset', 'ratio_total']], on = 'Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8d085",
   "metadata": {},
   "source": [
    "## Ratio requirements with both characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53678528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_quality['norm_ratio2'] = df_stats_quality['ratio']/df_stats_quality['ratio_total']\n",
    "df_stats_functional['norm_ratio2'] = df_stats_functional['ratio']/df_stats_functional['ratio_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8b31d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_quality_0 = df_stats_quality[df_stats_quality['correct'] == 0] \n",
    "df_stats_functional_0 = df_stats_functional[df_stats_functional['correct'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "184390c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_quality_0 =round(df_stats_quality_0.pivot_table(index=['type'], values=['norm_ratio2'],columns=['model','Dataset']),2)\n",
    "pivot_quality_0.columns = pivot_quality_0.columns.swaplevel(0, 1) \n",
    "pivot_quality_0 = pivot_quality_0.sort_index(axis=1)\n",
    "pivot_functional_0 =round(df_stats_functional_0.pivot_table(index=['type'], values=['norm_ratio2'],columns=['model','Dataset']),2)\n",
    "pivot_functional_0.columns = pivot_functional_0.columns.swaplevel(0, 1) \n",
    "pivot_functional_0 = pivot_functional_0.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25de9804",
   "metadata": {},
   "source": [
    " ## Accuracy per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6ed47d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_quality_1 = df_stats_quality[df_stats_quality['correct'] == 1] \n",
    "df_stats_functional_1 = df_stats_functional[df_stats_functional['correct'] == 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_quality_1.loc[:, ['rows']] = [64 if d == 'l' else 128 if d == 'o' else 192 if d in ['p','rds'] else -1 for d in df_stats_quality_1['Dataset']]\n",
    "df_stats_functional_1.loc[:, ['rows']] = [64 if d == 'l' else 128 if d == 'o' else 192 if d in ['p','rds'] else -1 for d in df_stats_functional_1['Dataset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "96c8fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_quality_1.loc[:, ['acc']] = df_stats_quality_1['RequirementText']/df_stats_quality_1['rows'] \n",
    "df_stats_datasets_q = df_stats_quality_1.groupby(['model', 'Dataset'])['acc'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "\n",
    "df_stats_functional_1.loc[:, ['acc']] = df_stats_functional_1['RequirementText']/df_stats_functional_1['rows']\n",
    "df_stats_datasets_f = df_stats_functional_1.groupby(['model', 'Dataset'])['acc'].agg(['mean', 'std', 'min', 'max']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('pivottable_stats.xlsx', engine='xlsxwriter') as writer:\n",
    "    pivot_quality_0.to_excel(writer, sheet_name='Quality')\n",
    "    pivot_functional_0.to_excel(writer, sheet_name='Functional')\n",
    "    round(df_stats_datasets_q.pivot_table(index='model', columns='Dataset', values=['mean','std', 'min', 'max']), 2).to_excel(writer, sheet_name='Acc_quality')\n",
    "    round(df_stats_datasets_f.pivot_table(index='model', columns='Dataset', values=['mean','std', 'min', 'max']), 2).to_excel(writer, sheet_name='Acc_functional')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_usst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
